{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense, Dropout,LSTM,Conv1D,Flatten,MaxPooling1D,UpSampling1D\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "import scipy.stats as stats\n",
    "from data_augmentation.augmentation import *\n",
    "from data_augmentation.helper import *\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import dbscan\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import umap\n",
    "import umap.plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H5 Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ELECTRODES = 32\n",
    "CUT_OFF = 120\n",
    "STEP_CUT_OFF = 25\n",
    "CYCLE_PER_SEC = 30000\n",
    "\n",
    "raw_stream = \"Data/Recording_0/AnalogStream/Stream_1/ChannelData\"\n",
    "electrode_tpl = \"Data/Recording_0/SegmentStream/Stream_0/SegmentData\"\n",
    "\n",
    "\n",
    "def find_sublist(sub, bigger):\n",
    "    if not bigger:\n",
    "        return -1\n",
    "    if not sub:\n",
    "        return 0\n",
    "    first, rest = sub[0], sub[1:]\n",
    "    pos = 0\n",
    "    try:\n",
    "        while True:\n",
    "            pos = bigger.index(first, pos) + 1\n",
    "            if not rest or bigger[pos:pos+len(rest)] == rest:\n",
    "                return pos\n",
    "    except ValueError:\n",
    "        return -1 \n",
    "\n",
    "def get_raw_electrode_data(path: str, electrode_number_start: int, electrode_number_stop: int,label: int) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for index in range(electrode_number_start, electrode_number_stop):\n",
    "        print(f'\\nNum electrode : {index}')\n",
    "        spike_windows = np.array(f[f'{electrode_tpl}_{index}'][()]).T\n",
    "        dataRaw = f[f'{raw_stream}'][index]\n",
    "        range_cut_off = []\n",
    "\n",
    "        K = len(spike_windows)\n",
    "        for indx,spke in enumerate(spike_windows):\n",
    "            tmp = find_sublist(spke.tolist(), dataRaw.tolist())\n",
    "            if(tmp != -1):\n",
    "                if(tmp - (CUT_OFF/2) >= 0):\n",
    "                    cut = CUT_OFF/2\n",
    "                    range_cut_off = dataRaw[tmp-cut:tmp+cut]\n",
    "                else:\n",
    "                    range_cut_off = dataRaw[tmp:tmp+CUT_OFF]\n",
    "\n",
    "                X.append(range_cut_off)\n",
    "                Y.append(label)    \n",
    "            print(end=\"\\r|%-80s|\" % (\"=\"*int(80*indx/(K-1))))\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def get_raw_data(path: str) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for index in range(10,13):\n",
    "        print(f'\\nNum electrode : {index}')\n",
    "        spike_windows = f[f'SpikeWindow-0.{index}'][()]\n",
    "        dataRaw = f[f'Raw-0.{index}'][0:len(f[f'Raw-0.{index}']):1, 1]\n",
    "        sp = []\n",
    "\n",
    "        K = len(spike_windows)\n",
    "        for indx,spke in enumerate(spike_windows):\n",
    "            tmp = find_sublist(spke.tolist(), dataRaw.tolist())\n",
    "            if(tmp != -1):\n",
    "                sp.append(tmp + 30)\n",
    "            print(end=\"\\r|%-80s|\" % (\"=\"*int(80*indx/(K-1))))\n",
    "        \n",
    "        for i in range(0,len(dataRaw)-CUT_OFF,CUT_OFF):\n",
    "            range_cut_off = dataRaw[i:i+CUT_OFF]\n",
    "            if(any(x in sp for x in range(i,i+CUT_OFF))):\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "            X.append(range_cut_off)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def get_noise_data(path: str,shape:int,arr: np.ndarray) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for index in arr:\n",
    "        print(f'\\nNum electrode : {index}')\n",
    "        spike_windows = f[f'SpikeWindow-0.{index}'][()]\n",
    "        print(len(spike_windows))\n",
    "        dataRaw = f[f'Raw-0.{index}'][0:len(f[f'Raw-0.{index}']):1, 1]\n",
    "        sp = []\n",
    "\n",
    "        K = len(spike_windows)\n",
    "        for indx,spke in enumerate(spike_windows):\n",
    "            tmp = find_sublist(spke.tolist(), dataRaw.tolist())\n",
    "            if(tmp != -1):\n",
    "                sp.append(tmp + 30)\n",
    "            print(end=\"\\r|%-80s|\" % (\"=\"*int(80*indx/(K-1))))\n",
    "        \n",
    "        for i in range(0,len(dataRaw)-CUT_OFF,CUT_OFF):\n",
    "            if(len(X) == shape):\n",
    "                return X\n",
    "            range_cut_off = dataRaw[i:i+CUT_OFF]\n",
    "            if(not any(x in sp for x in range(i,i+CUT_OFF))):\n",
    "                X.append(range_cut_off)\n",
    "\n",
    "\n",
    "\n",
    "def get_spike_data(path: str, arr: np.ndarray) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    sp = []\n",
    "\n",
    "    for index in arr:\n",
    "        print(f'\\nNum electrode : {index}')\n",
    "        spike_windows = f[f'SpikeWindow-0.{index}'][()]\n",
    "\n",
    "        K = len(spike_windows)\n",
    "        print(K)\n",
    "        for indx,spke in enumerate(spike_windows):\n",
    "            sp.append(spke[0:CUT_OFF])\n",
    "            if(K != 1):\n",
    "                print(end=\"\\r|%-80s|\" % (\"=\"*int(80*indx/(K-1))))\n",
    "    return sp\n",
    "\n",
    "def show_spike_data(path: str, number_by_fold:int) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    for n in f.keys():\n",
    "        if(\"SpikeWindow-0.\" in n):\n",
    "            spike_windows = f[n][()]\n",
    "            fig, axs = plt.subplots(number_by_fold)\n",
    "\n",
    "            for i in range(number_by_fold):\n",
    "                fig.set_size_inches(10, 5)\n",
    "                axs[i].plot(spike_windows[i])     \n",
    "\n",
    "def show_multiple_file_Spike(directory: str):\n",
    "    for filename in os.listdir(directory):\n",
    "        print(f\"{filename}\")\n",
    "        show_spike_data(os.path.join(directory, filename),5)  \n",
    "                \n",
    "def get_number_spike_raw_data(path: str) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    event = 0\n",
    "    for n in f.keys():\n",
    "        if(\"SpikeTimestamp-0\" in n):\n",
    "            event += f[n].shape[0]\n",
    "    return event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_spike = False\n",
    "rebuild_noise = False\n",
    "\n",
    "if os.path.exists(\"x_spike\"+str(CUT_OFF)+\".csv\") and not rebuild_spike:\n",
    "    spike = np.genfromtxt(\"x_spike\"+str(CUT_OFF)+\".csv\", delimiter=',')\n",
    "else:\n",
    "    spike = get_spike_data('./RAW/2022-12-09T11-44-00_SpikeOnChip_SPOC1_Data.h5',[14,27,29])\n",
    "    np.savetxt(\"x_spike\"+str(CUT_OFF)+\".csv\", spike, delimiter=\",\")\n",
    "\n",
    "if os.path.exists(\"x_noise\"+str(CUT_OFF)+\".csv\") and os.path.exists(\"x_tbi\"+str(CUT_OFF)+\".csv\") and not rebuild_noise:\n",
    "    noise = np.genfromtxt(\"x_noise\"+str(CUT_OFF)+\".csv\", delimiter=',')\n",
    "    tbi_flat = np.genfromtxt(\"x_tbi\"+str(CUT_OFF)+\".csv\", delimiter=',')\n",
    "else:\n",
    "    noise = get_noise_data('./RAW/2022-11-23T16-07-00_SpikeOnChip_SPOC1_Data.h5',len(spike),[1,3,5])\n",
    "    tbi = []\n",
    "    for i in range(32):\n",
    "        if(i != 6 and i != 7 and i != 25):\n",
    "            tbi.append(get_spike_data('./Post TBI 1/2022-11-23T16-30-00_SpikeOnChip_SPOC1_Data.h5',[i]))\n",
    "    tbi_flat = [item for sublist in tbi for item in sublist]\n",
    "    np.savetxt(\"x_noise\"+str(CUT_OFF)+\".csv\", noise, delimiter=\",\")\n",
    "    np.savetxt(\"x_tbi\"+str(CUT_OFF)+\".csv\", tbi_flat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some exemple for a spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "tmp = 0\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "for row in spike:\n",
    "    if(item == 4):\n",
    "        break\n",
    "    fig.set_size_inches(20, 5)\n",
    "    if(item == 2):\n",
    "        tmp += 1 \n",
    "    axs[tmp,item%2].plot(row)\n",
    "    item += 1\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show noise sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "tmp = 0\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "for row in noise:\n",
    "    if(item == 4):\n",
    "        break\n",
    "    fig.set_size_inches(20, 5)\n",
    "    if(item == 2):\n",
    "        tmp += 1 \n",
    "    axs[tmp,item%2].plot(row)\n",
    "    item += 1\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot info from spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_long_waves_df(waves, labels):\n",
    "    spikes_df = pd.DataFrame(waves, columns=[\"time{}\".format(x) for x in range(waves.shape[1])])\n",
    "    spikes_df['label'] = labels\n",
    "\n",
    "    spikes_df_long = pd.melt(spikes_df, id_vars=['label'], value_vars=None, var_name='timepoint', )\n",
    "    spikes_df_long['timepoint'] = spikes_df_long.timepoint.apply(lambda name: int(name[4:]))\n",
    "    return spikes_df_long\n",
    "\n",
    "spikes_df_long = build_long_waves_df(np.array(spike), 'spike')\n",
    "sns.lineplot(x='timepoint', y='value', data=spikes_df_long, ci='sd', hue='label', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.DataFrame(spike), pd.DataFrame(noise),pd.DataFrame(tbi_flat)], axis=0)\n",
    "y = np.append(np.ones(len(spike)),np.zeros(len(noise) + len(tbi_flat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spike = pd.DataFrame(spike)\n",
    "y_spike = np.ones(df_spike.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection as ms\n",
    "\n",
    "#define train and test split\n",
    "\n",
    "x_train, x_test, y_train, y_test = ms.train_test_split(df, y, \n",
    "                                     test_size=0.20, random_state=1)\n",
    "\n",
    "x_train_spike, x_test_spike, y_train_spike, y_test_spike = ms.train_test_split(df_spike, y_spike, \n",
    "                                     test_size=0.20, random_state=1)\n",
    "\n",
    "print(\"---------------- Dataset ------------------\")\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "print(\"------------- Dataset Spike ---------------\")\n",
    "print(x_train_spike.shape, x_test_spike.shape, y_train_spike.shape, y_test_spike.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = x_train.shape[1]\n",
    "\n",
    "input_img = Input(shape=(n_inputs,))\n",
    "encoded = Dense(240, activation='relu')(input_img)\n",
    "encoded = Dense(120, activation='relu')(encoded)\n",
    "encoded = Dense(60, activation='relu')(encoded)\n",
    "# encoded = Dense(16, activation='relu')(encoded)\n",
    "# encoded = Dense(8, activation='relu')(encoded)\n",
    "# encoded = Dense(4, activation='relu')(encoded)\n",
    "# decoded = Dense(4, activation='relu')(encoded)\n",
    "# decoded = Dense(8, activation='relu')(decoded)\n",
    "# decoded = Dense(16, activation='relu')(decoded)\n",
    "decoded = Dense(60, activation='relu')(encoded)\n",
    "decoded = Dense(120, activation='relu')(decoded)\n",
    "decoded = Dense(240, activation='relu')(decoded)\n",
    "decoded = Dense(n_inputs, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.summary()\n",
    "autoencoder.compile(optimizer='SGD', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(autoencoder, to_file='./Denoising_Autoencoder.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = True\n",
    "\n",
    "if os.path.exists(\"denoising_Dense_2.h5\") and not train:\n",
    "    autoencoder = tf.keras.models.load_model('denoising_Dense_2.h5')\n",
    "else:\n",
    "    # checkpoint_filepath = './checkpoint/auto'\n",
    "    from keras import backend as K\n",
    "    K.set_value(autoencoder.optimizer.learning_rate, 0.1)\n",
    "    history = autoencoder.fit(\n",
    "        x_train_spike,\n",
    "        x_train_spike,\n",
    "        epochs=120,\n",
    "        batch_size=32,\n",
    "        validation_split=0.15,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=25, mode=\"min\"),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # plot loss\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    autoencoder.save('denoising_Dense_2.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS = [5, 15, 25, 50, 100, 200]\n",
    "MIN_DISTS = [0.1, 0.25, 0.5, 0.8, 0.99]\n",
    "\n",
    "def build_all_mappers(data):\n",
    "    mappers = []\n",
    "    for n in tqdm(N_NEIGHBORS):\n",
    "        for d in tqdm(MIN_DISTS, leave=False):\n",
    "            path = f'./model/mapper-{n}-{d}'\n",
    "            try:\n",
    "                mapper = umap.UMAP(n_neighbors=n, min_dist=d).fit(data)\n",
    "                mappers.append(mapper)\n",
    "                joblib.dump(mapper, path)\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "    return mappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoencoder= tf.keras.models.load_model('denoising_Dense_2.h5')\n",
    "output_layer = (autoencoder.layers[-5].output)\n",
    "\n",
    "encoder = Model(autoencoder.input, output_layer)\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_encode = encoder.predict(x_train)\n",
    "X_test_encode = encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Build mappers form data\")\n",
    "\n",
    "mappers = build_all_mappers(X_train_encode)\n",
    "for mapper in mappers[::5]:\n",
    "    umap.plot.points(mapper,labels=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encode = autoencoder.predict(x_train)\n",
    "X_test_encode = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Build mappers form data\")\n",
    "\n",
    "mappers = build_all_mappers(X_train_encode)\n",
    "for mapper in mappers[::5]:\n",
    "    umap.plot.points(mapper,labels=y_train)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "937fe4b5d9b071df378a454ac50205e265fd9fba8afe26b93261d5b99b0f19da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
