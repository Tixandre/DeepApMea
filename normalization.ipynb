{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import dbscan\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import h5py\n",
    "\n",
    "# embedding\n",
    "import umap\n",
    "import umap.plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUT_OFF = 120\n",
    "\n",
    "if os.path.exists(\"x_spike\"+str(CUT_OFF)+\".csv\"):\n",
    "    spike = np.genfromtxt(\"x_spike\"+str(CUT_OFF)+\".csv\", delimiter=',')\n",
    "\n",
    "if os.path.exists(\"x_noise\"+str(CUT_OFF)+\".csv\") and os.path.exists(\"x_tbi\"+str(CUT_OFF)+\".csv\"):\n",
    "    noise = np.genfromtxt(\"x_noise\"+str(CUT_OFF)+\".csv\", delimiter=',')\n",
    "    tbi_flat = np.genfromtxt(\"x_tbi\"+str(CUT_OFF)+\".csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.DataFrame(spike), pd.DataFrame(noise),pd.DataFrame(tbi_flat)], axis=0)\n",
    "y = np.append(np.ones(len(spike)),np.zeros(len(noise) + len(tbi_flat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NEIGHBORS = [5, 15, 25, 50, 100, 200]\n",
    "MIN_DISTS = [0.1, 0.25, 0.5, 0.8, 0.99]\n",
    "\n",
    "def plot_waves(waves, index_list, ncol=5, title=False):\n",
    "    nrow = np.ceil(len(index_list) / ncol)\n",
    "\n",
    "    plt.figure(figsize=(16, 3.5 * nrow))\n",
    "    for i, idx in enumerate(index_list):\n",
    "        args = {} if not title else {'title': idx}\n",
    "        plt.subplot(nrow, ncol, i+1, **args)\n",
    "        plt.plot(waves[idx], 'k-')\n",
    "        plt.grid()\n",
    "\n",
    "def build_all_mappers(data,norm):\n",
    "    mappers = []\n",
    "    for n in tqdm(N_NEIGHBORS):\n",
    "        for d in tqdm(MIN_DISTS, leave=False):\n",
    "            path = f'./model/mapper-{norm}-{n}-{d}'\n",
    "            try:\n",
    "                mapper = umap.UMAP(n_neighbors=n, min_dist=d).fit(data)\n",
    "                mappers.append(mapper)\n",
    "                joblib.dump(mapper, path)\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "    return mappers\n",
    "            \n",
    "def load_all_mappers():\n",
    "    mappers = Parallel(n_jobs=32)(delayed(joblib.load)(f'./model/mapper-{n}-{d}') for n in N_NEIGHBORS for d in MIN_DISTS)\n",
    "    return mappers\n",
    "\n",
    "def plot_spikes_cluster(labels,dataset_train, ncol=3):\n",
    "    print(labels)\n",
    "    labels_uniq = np.unique(labels)\n",
    "    nrow = np.ceil(len(labels_uniq) / ncol)\n",
    "\n",
    "    plt.figure(figsize=(16, 3.5 * nrow))\n",
    "    for i, label in enumerate(labels_uniq):\n",
    "        plt.subplot(nrow, ncol, i+1, title=f\"class: {label}\")\n",
    "        plt.plot(dataset_train[labels==label].T, 'k-')\n",
    "        plt.grid()\n",
    "        \n",
    "def plot_spikes_samples(labels, clas,dataset_train, n_sample):\n",
    "    assert clas in np.unique(labels), f\"clas {clas} not in labels\"\n",
    "    \n",
    "    clas_idx = np.where(labels == clas)[0]\n",
    "    plot_waves(dataset_train, clas_idx[::len(clas_idx) // n_sample+1])\n",
    "    \n",
    "def umap_plot(mapper, labels, **args):\n",
    "    if 'theme' not in args:\n",
    "        args['theme'] = 'darkblue'\n",
    "    umap.plot.points(mapper, labels=labels, width=1200, height=1000, **args)\n",
    "    \n",
    "#------ Post-clustering template-plotting ------#\n",
    "    \n",
    "def build_long_waves_df(waves, labels):\n",
    "    spikes_df = pd.DataFrame(waves.numpy(), columns=[\"time{}\".format(x) for x in range(waves.shape[1])])\n",
    "    spikes_df['label'] = labels\n",
    "\n",
    "    spikes_df_long = pd.melt(spikes_df, id_vars=['label'], value_vars=None, var_name='timepoint', )\n",
    "    spikes_df_long['timepoint'] = spikes_df_long.timepoint.apply(lambda name: int(name[4:]))\n",
    "    return spikes_df_long\n",
    "\n",
    "def plot_templates(waves_df_long, ncol=3, verbose=False):\n",
    "    assert 'label' in waves_df_long\n",
    "    labels_uniq = sorted(waves_df_long.label.unique())\n",
    "    nrow = np.ceil(len(labels_uniq) / ncol)\n",
    "\n",
    "    plt.figure(figsize=(16, 4 * nrow))\n",
    "    for i, label in tqdm(enumerate(labels_uniq, start=1), total=len(labels_uniq)):\n",
    "        if verbose: print(f\"{label} -> {(waves_df_long.label == label).sum()}\")\n",
    "        plt.subplot(nrow, ncol, i, title=f\"class: {label}\")\n",
    "        sns.lineplot(x='timepoint', y='value', data=waves_df_long[waves_df_long.label==label], ci='sd').set_xlabel(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# On a fresh run if we don't have/want to compute \"sub-par\" mappers\n",
    "\n",
    "\n",
    "distributions = [\n",
    "    df.to_numpy(),\n",
    "    StandardScaler().fit_transform(df),\n",
    "    MinMaxScaler().fit_transform(df),\n",
    "    MaxAbsScaler().fit_transform(df),\n",
    "    RobustScaler(quantile_range=(25, 75)).fit_transform(df),\n",
    "    PowerTransformer(method=\"yeo-johnson\").fit_transform(df),\n",
    "    QuantileTransformer(output_distribution=\"uniform\").fit_transform(df),\n",
    "    QuantileTransformer(output_distribution=\"normal\").fit_transform(df),\n",
    "    Normalizer().fit_transform(df),\n",
    "]\n",
    "normes = [\n",
    "    'None',\n",
    "    'StandardScaler',\n",
    "    'MinMaxScaler',\n",
    "    'MaxAbsScaler',\n",
    "    'RobustScaler',\n",
    "    'PowerTransformerYeoJohnson',\n",
    "    'QuantileTransformerUniform',\n",
    "    'QuantileTransformerNormal',\n",
    "    'Normalizer',\n",
    "]\n",
    "\n",
    "# if os.path.exists(\"./model/mapper-25-0.1\") and not train:\n",
    "#     print(\"loading mappers from disk\")\n",
    "#     mappers = load_all_mappers()\n",
    "#     mapper = mappers[10]\n",
    "# else:\n",
    "\n",
    "tmp = 0\n",
    "fig, axs = plt.subplots(9, 3,constrained_layout = True)\n",
    "fig.set_size_inches(10, 10)\n",
    "for idx,i in enumerate(distributions):\n",
    "    item = 0\n",
    "    for row in i:\n",
    "        if(item == 3):\n",
    "            break\n",
    "        axs[tmp,item%3].set_title(\"Norm with \" + normes[idx])\n",
    "        axs[tmp,item%3].plot(row)\n",
    "        item += 1\n",
    "    tmp += 1\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /!\\ long /!\\\n",
    "# %time build_all_mappers(all_spikes)\n",
    "\n",
    "\n",
    "for idx,i in enumerate(distributions):\n",
    "    print(\"Build mappers form data\")\n",
    "    mappers = build_all_mappers(i,normes[idx])\n",
    "    for mapper in mappers[::5]:\n",
    "        umap.plot.points(mapper,labels=y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "937fe4b5d9b071df378a454ac50205e265fd9fba8afe26b93261d5b99b0f19da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
