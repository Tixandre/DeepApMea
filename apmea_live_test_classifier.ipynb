{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense, Dropout,LSTM,Conv1D,Flatten,MaxPooling1D,UpSampling1D,BatchNormalization,Bidirectional\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras.utils import plot_model\n",
    "import scipy.stats as stats\n",
    "from data_augmentation.augmentation import *\n",
    "from data_augmentation.helper import *\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import dbscan\n",
    "import joblib\n",
    "from tqdm.notebook import tqdm\n",
    "import keras_tuner\n",
    "\n",
    "import umap\n",
    "import umap.plot\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H5 Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ELECTRODES = 32\n",
    "CUT_OFF = 120\n",
    "STEP_CUT_OFF = 25\n",
    "CYCLE_PER_SEC = 30000\n",
    "\n",
    "raw_stream = \"Data/Recording_0/AnalogStream/Stream_1/ChannelData\"\n",
    "electrode_tpl = \"Data/Recording_0/SegmentStream/Stream_0/SegmentData\"\n",
    "\n",
    "\n",
    "def find_sublist(sub, bigger):\n",
    "    if not bigger:\n",
    "        return -1\n",
    "    if not sub:\n",
    "        return 0\n",
    "    first, rest = sub[0], sub[1:]\n",
    "    pos = 0\n",
    "    try:\n",
    "        while True:\n",
    "            pos = bigger.index(first, pos) + 1\n",
    "            if not rest or bigger[pos:pos+len(rest)] == rest:\n",
    "                return pos\n",
    "    except ValueError:\n",
    "        return -1 \n",
    "\n",
    "def get_raw_electrode_data(path: str, electrode_number_start: int, electrode_number_stop: int,label: int) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for index in range(electrode_number_start, electrode_number_stop):\n",
    "        print(f'\\nNum electrode : {index}')\n",
    "        spike_windows = np.array(f[f'{electrode_tpl}_{index}'][()]).T\n",
    "        dataRaw = f[f'{raw_stream}'][index]\n",
    "        range_cut_off = []\n",
    "\n",
    "        K = len(spike_windows)\n",
    "        for indx,spke in enumerate(spike_windows):\n",
    "            tmp = find_sublist(spke.tolist(), dataRaw.tolist())\n",
    "            if(tmp != -1):\n",
    "                if(tmp - (CUT_OFF/2) >= 0):\n",
    "                    cut = CUT_OFF/2\n",
    "                    range_cut_off = dataRaw[tmp-cut:tmp+cut]\n",
    "                else:\n",
    "                    range_cut_off = dataRaw[tmp:tmp+CUT_OFF]\n",
    "\n",
    "                X.append(range_cut_off)\n",
    "                Y.append(label)    \n",
    "            print(end=\"\\r|%-80s|\" % (\"=\"*int(80*indx/(K-1))))\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def get_raw_data(path: str) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for index in range(10,13):\n",
    "        print(f'\\nNum electrode : {index}')\n",
    "        spike_windows = f[f'SpikeWindow-0.{index}'][()]\n",
    "        dataRaw = f[f'Raw-0.{index}'][0:len(f[f'Raw-0.{index}']):1, 1]\n",
    "        sp = []\n",
    "\n",
    "        K = len(spike_windows)\n",
    "        for indx,spke in enumerate(spike_windows):\n",
    "            tmp = find_sublist(spke.tolist(), dataRaw.tolist())\n",
    "            if(tmp != -1):\n",
    "                sp.append(tmp + 30)\n",
    "            print(end=\"\\r|%-80s|\" % (\"=\"*int(80*indx/(K-1))))\n",
    "        \n",
    "        for i in range(0,len(dataRaw)-CUT_OFF,CUT_OFF):\n",
    "            range_cut_off = dataRaw[i:i+CUT_OFF]\n",
    "            if(any(x in sp for x in range(i,i+CUT_OFF))):\n",
    "                Y.append(1)\n",
    "            else:\n",
    "                Y.append(0)\n",
    "            X.append(range_cut_off)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def get_noise_data(path: str,shape:int,arr: np.ndarray) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for index in arr:\n",
    "        print(f'\\nNum electrode : {index}')\n",
    "        spike_windows = f[f'SpikeWindow-0.{index}'][()]\n",
    "        print(len(spike_windows))\n",
    "        dataRaw = f[f'Raw-0.{index}'][0:len(f[f'Raw-0.{index}']):1, 1]\n",
    "        sp = []\n",
    "\n",
    "        K = len(spike_windows)\n",
    "        for indx,spke in enumerate(spike_windows):\n",
    "            tmp = find_sublist(spke.tolist(), dataRaw.tolist())\n",
    "            if(tmp != -1):\n",
    "                sp.append(tmp + 30)\n",
    "            print(end=\"\\r|%-80s|\" % (\"=\"*int(80*indx/(K-1))))\n",
    "        \n",
    "        for i in range(0,len(dataRaw)-CUT_OFF,CUT_OFF):\n",
    "            if(len(X) == shape):\n",
    "                return X\n",
    "            range_cut_off = dataRaw[i:i+CUT_OFF]\n",
    "            if(not any(x in sp for x in range(i,i+CUT_OFF))):\n",
    "                X.append(range_cut_off)\n",
    "\n",
    "\n",
    "\n",
    "def get_spike_data(path: str, arr: np.ndarray) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    sp = []\n",
    "\n",
    "    for index in arr:\n",
    "        print(f'\\nNum electrode : {index}')\n",
    "        spike_windows = f[f'SpikeWindow-0.{index}'][()]\n",
    "\n",
    "        K = len(spike_windows)\n",
    "        print(K)\n",
    "        for indx,spke in enumerate(spike_windows):\n",
    "            sp.append(spke[0:CUT_OFF])\n",
    "            if(K != 1):\n",
    "                print(end=\"\\r|%-80s|\" % (\"=\"*int(80*indx/(K-1))))\n",
    "    return sp\n",
    "\n",
    "def show_spike_data(path: str, number_by_fold:int) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    for n in f.keys():\n",
    "        if(\"SpikeWindow-0.\" in n):\n",
    "            spike_windows = f[n][()]\n",
    "            fig, axs = plt.subplots(number_by_fold)\n",
    "\n",
    "            for i in range(number_by_fold):\n",
    "                fig.set_size_inches(10, 5)\n",
    "                axs[i].plot(spike_windows[i])     \n",
    "\n",
    "def show_multiple_file_Spike(directory: str):\n",
    "    for filename in os.listdir(directory):\n",
    "        print(f\"{filename}\")\n",
    "        show_spike_data(os.path.join(directory, filename),5)  \n",
    "                \n",
    "def get_number_spike_raw_data(path: str) -> np.ndarray:\n",
    "    f = h5py.File(path, mode='r')\n",
    "    event = 0\n",
    "    for n in f.keys():\n",
    "        if(\"SpikeTimestamp-0\" in n):\n",
    "            event += f[n].shape[0]\n",
    "    return event\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_spike = False\n",
    "rebuild_noise = False\n",
    "\n",
    "if os.path.exists(\"x_spike\"+str(CUT_OFF)+\".csv\") and not rebuild_spike:\n",
    "    spike = np.genfromtxt(\"x_spike\"+str(CUT_OFF)+\".csv\", delimiter=',')\n",
    "else:\n",
    "    spike = get_spike_data('./RAW/2022-12-09T11-44-00_SpikeOnChip_SPOC1_Data.h5',[14,27,29])\n",
    "    np.savetxt(\"x_spike\"+str(CUT_OFF)+\".csv\", spike, delimiter=\",\")\n",
    "\n",
    "if os.path.exists(\"x_noise\"+str(CUT_OFF)+\".csv\") and os.path.exists(\"x_tbi\"+str(CUT_OFF)+\".csv\") and not rebuild_noise:\n",
    "    noise = np.genfromtxt(\"x_noise\"+str(CUT_OFF)+\".csv\", delimiter=',')\n",
    "    tbi_flat = np.genfromtxt(\"x_tbi\"+str(CUT_OFF)+\".csv\", delimiter=',')\n",
    "else:\n",
    "    noise = get_noise_data('./RAW/2022-11-23T16-07-00_SpikeOnChip_SPOC1_Data.h5',len(spike),[1,3,5])\n",
    "    tbi = []\n",
    "    for i in range(32):\n",
    "        if(i != 6 and i != 7 and i != 25):\n",
    "            tbi.append(get_spike_data('./Post TBI 1/2022-11-23T16-30-00_SpikeOnChip_SPOC1_Data.h5',[i]))\n",
    "    \n",
    "    for i in range(32):\n",
    "        if(i != 7 and i != 14 and i != 26):\n",
    "            tbi.append(get_spike_data('./Post TBI 2/2022-11-23T16-42-00_SpikeOnChip_SPOC1_Data.h5',[i]))\n",
    "\n",
    "    tbi_flat = [item for sublist in tbi for item in sublist]\n",
    "    np.savetxt(\"x_noise\"+str(CUT_OFF)+\".csv\", noise, delimiter=\",\")\n",
    "    np.savetxt(\"x_tbi\"+str(CUT_OFF)+\".csv\", tbi_flat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show some exemple for a spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "tmp = 0\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "for row in spike:\n",
    "    if(item == 4):\n",
    "        break\n",
    "    fig.set_size_inches(20, 5)\n",
    "    if(item == 2):\n",
    "        tmp += 1 \n",
    "    axs[tmp,item%2].plot(row)\n",
    "    item += 1\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show noise sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "tmp = 0\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "for row in noise:\n",
    "    if(item == 4):\n",
    "        break\n",
    "    fig.set_size_inches(20, 5)\n",
    "    if(item == 2):\n",
    "        tmp += 1 \n",
    "    axs[tmp,item%2].plot(row)\n",
    "    item += 1\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_long_waves_df(waves, labels):\n",
    "    spikes_df = pd.DataFrame(waves, columns=[\"time{}\".format(x) for x in range(waves.shape[1])])\n",
    "    spikes_df['label'] = labels\n",
    "\n",
    "    spikes_df_long = pd.melt(spikes_df, id_vars=['label'], value_vars=None, var_name='timepoint', )\n",
    "    spikes_df_long['timepoint'] = spikes_df_long.timepoint.apply(lambda name: int(name[4:]))\n",
    "    return spikes_df_long\n",
    "\n",
    "spikes_df_long = build_long_waves_df(np.array(spike), 'spike')\n",
    "sns.lineplot(x='timepoint', y='value', data=spikes_df_long, ci='sd', hue='label', legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.DataFrame(spike), pd.DataFrame(noise),pd.DataFrame(tbi_flat)], axis=0)\n",
    "y = np.append(np.ones(len(spike)),np.zeros(len(noise) + len(tbi_flat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spike = pd.DataFrame(spike)\n",
    "y_spike = np.ones(df_spike.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection as ms\n",
    "\n",
    "x_train, x_test, y_train, y_test = ms.train_test_split(df, y, \n",
    "                                     test_size=0.20, random_state=1)\n",
    "\n",
    "x_train_spike, x_test_spike, y_train_spike, y_test_spike = ms.train_test_split(df_spike, y_spike, \n",
    "                                     test_size=0.20, random_state=1)\n",
    "\n",
    "print(\"---------------- Dataset ------------------\")\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "print(\"------------- Dataset Spike ---------------\")\n",
    "print(x_train_spike.shape, x_test_spike.shape, y_train_spike.shape, y_test_spike.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"denoising_Dense_1.h5\"):\n",
    "    autoencoder = tf.keras.models.load_model('denoising_Dense_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = (autoencoder.layers[-11].output)\n",
    "\n",
    "encoder = Model(autoencoder.input, output_layer)\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train_encode = encoder.predict(x_train)\n",
    "X_test_encode = encoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    clf = make_pipeline(StandardScaler(), clf)\n",
    "    clf.fit(X_train_encode, y_train)\n",
    "    y_pred = clf.predict(X_test_encode)\n",
    "\n",
    "    print(f'{name} & {accuracy_score(y_test, y_pred).round(4)} & {recall_score(y_test, y_pred).round(4)} & {precision_score(y_test, y_pred).round(4)} & {f1_score(y_test, y_pred).round(4)}')\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "    \n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Actuals', fontsize=18)\n",
    "    plt.title('Confusion Matrix', fontsize=18)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "model = KNeighborsClassifier()\n",
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid['n_neighbors'] = [3,5,11,19,21]\n",
    "grid['weights'] = ['uniform','distance']\n",
    "grid['metric'] = ['euclidean','manhattan']\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X_train_encode, y_train)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), KNeighborsClassifier(metric='manhattan',n_neighbors=5,weights='uniform'))\n",
    "clf.fit(X_train_encode, y_train)\n",
    "y_pred = clf.predict(X_test_encode)\n",
    "\n",
    "print(f'Nearest Neighbors & {accuracy_score(y_test, y_pred).round(4)} & {recall_score(y_test, y_pred).round(4)} & {precision_score(y_test, y_pred).round(4)} & {f1_score(y_test, y_pred).round(4)}')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "    \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "model = SVC()\n",
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid['C'] = [0.1, 1, 10, 100, 1000]\n",
    "grid['gamma'] = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "grid['kernel'] = ['linear']\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X_train_encode, y_train)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(kernel='linear',C=1,gamma=1))\n",
    "clf.fit(X_train_encode, y_train)\n",
    "y_pred = clf.predict(X_test_encode)\n",
    "\n",
    "print(f'Linear SVM & {accuracy_score(y_test, y_pred).round(4)} & {recall_score(y_test, y_pred).round(4)} & {precision_score(y_test, y_pred).round(4)} & {f1_score(y_test, y_pred).round(4)}')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "    \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "model = MLPClassifier(max_iter=1000)\n",
    "# define the grid of values to search\n",
    "grid = {\n",
    "    'hidden_layer_sizes': [(10,30,10),(20,),(10,),(100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05,1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X_train_encode, y_train)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000,activation='tanh',alpha=0.0001,hidden_layer_sizes=(100,),learning_rate='adaptive',solver='adam'))\n",
    "clf.fit(X_train_encode, y_train)\n",
    "y_pred = clf.predict(X_test_encode)\n",
    "\n",
    "print(f'Neural Net & {accuracy_score(y_test, y_pred).round(4)} & {recall_score(y_test, y_pred).round(4)} & {precision_score(y_test, y_pred).round(4)} & {f1_score(y_test, y_pred).round(4)}')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "    \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "model = AdaBoostClassifier()\n",
    "# define the grid of values to search\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [10, 50, 100, 500]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy')\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X_train_encode, y_train)\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# summarize all scores that were evaluated\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), AdaBoostClassifier(learning_rate=1.0,n_estimators=500))\n",
    "clf.fit(X_train_encode, y_train)\n",
    "y_pred = clf.predict(X_test_encode)\n",
    "\n",
    "print(f'AdaBoost & {accuracy_score(y_test, y_pred).round(4)} & {recall_score(y_test, y_pred).round(4)} & {precision_score(y_test, y_pred).round(4)} & {f1_score(y_test, y_pred).round(4)}')\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "    \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_dense(hp):\n",
    "    n_inputs = X_train_encode.shape[1]\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(Input(shape=(n_inputs,)))\n",
    "    model.add(Dense(units=hp.Int(\"units0\", min_value=240, max_value=960, step=120),\n",
    "      activation='relu'))\n",
    "    model.add(Dense(units=hp.Int(\"units1\", min_value=360, max_value=960, step=120),\n",
    "      activation='relu'))\n",
    "    model.add(Dense(units=hp.Int(\"units2\", min_value=200, max_value=960, step=100),\n",
    "      activation='relu'))\n",
    "    model.add(Dense(units=hp.Int(\"units3\", min_value=60, max_value=240, step=60),\n",
    "      activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='SGD',loss='mse')\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "if train:\n",
    "    tuner = keras_tuner.RandomSearch(\n",
    "        build_model_dense,\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        project_name=\"Classifier_Dense\")\n",
    "    tuner.search(X_train_encode, y_train, epochs=10, callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001),\n",
    "        ], validation_split=0.15)\n",
    "    bestHP = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    # build the best model and train it\n",
    "    print(\"[INFO] training the best model...\")\n",
    "    model = tuner.hypermodel.build(bestHP)\n",
    "    H = model.fit(x=X_train_encode, y=y_train,\n",
    "        validation_split=0.15,\n",
    "        epochs=50, callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001),\n",
    "        ], verbose=1)\n",
    "else:\n",
    "    if os.path.exists('DENSE_Classifier.h5'):\n",
    "        model = tf.keras.models.load_model('DENSE_Classifier.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(x=X_test_encode, batch_size=32).round()\n",
    "print(classification_report(y_test,predictions))\n",
    "# generate the training loss/accuracy plot\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=predictions)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "   \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history['loss'], label='train')\n",
    "plt.plot(H.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Custom dense classifier & {accuracy_score(y_test, predictions).round(4)} & {recall_score(y_test, predictions).round(4)} & {precision_score(y_test, predictions).round(4)} & {f1_score(y_test, predictions).round(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "tmp = 0\n",
    "idx = 0\n",
    "\n",
    "idx_m = 0\n",
    "idx_p = 0\n",
    "\n",
    "fig, axs = plt.subplots(4)\n",
    "for index,row in x_test.iterrows():\n",
    "    fig.set_size_inches(15, 20)\n",
    "    if(idx == 4):\n",
    "        break\n",
    "    if(predictions[item] == 0 and y_test[item] == 1 and idx_m < 2):\n",
    "        axs[idx].plot(row)\n",
    "        axs[idx].set_title(\"Predicted Noise | Actuals Spike\")\n",
    "        idx += 1\n",
    "        idx_m+=1\n",
    "    if(predictions[item] == 1 and y_test[item] == 0  and idx_p < 2):\n",
    "        axs[idx].plot(row)\n",
    "        axs[idx].set_title(\"Predicted Spike | Actuals Noise\")\n",
    "        idx += 1\n",
    "        idx_p+=1\n",
    "\n",
    "    item += 1\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cnn(hp):\n",
    "    n_inputs = X_train_encode.shape[1]\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(Input(shape=(n_inputs,1)))\n",
    "    model.add(Conv1D(filters = hp.Choice('cnn1', [32,64,128,256]), kernel_size = hp.Choice('kernel1', [3,5,7,9]), activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters = hp.Choice('cnn2', [32,64,128,256]), kernel_size = hp.Choice('kernel2', [3,5,7,9]), activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(filters = hp.Choice('cnn3', [32,64,128,256]), kernel_size = hp.Choice('kernel3', [3,5,7,9]), activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=50, activation='relu'))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with Adam optimizer\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                metrics=['accuracy'],\n",
    "                optimizer='Adam')\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "if train:\n",
    "    tuner = keras_tuner.RandomSearch(\n",
    "        build_model_cnn,\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        project_name=\"Classifier_CNN\")\n",
    "    tuner.search(X_train_encode, y_train, epochs=10, callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001),\n",
    "        ], validation_split=0.15)\n",
    "    bestHP = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    # build the best model and train it\n",
    "    print(\"[INFO] training the best model...\")\n",
    "    model = tuner.hypermodel.build(bestHP)\n",
    "    H = model.fit(x=X_train_encode, y=y_train,\n",
    "        validation_split=0.15,\n",
    "        epochs=50, callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=7, mode=\"min\"),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001),\n",
    "        ], verbose=1)\n",
    "else:\n",
    "    if os.path.exists('CNN_Classifier.h5'):\n",
    "        model = tf.keras.models.load_model('CNN_Classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(x=X_test_encode, batch_size=32).round()\n",
    "print(classification_report(y_test,predictions))\n",
    "# generate the training loss/accuracy plot\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=predictions)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "   \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history['loss'], label='train')\n",
    "plt.plot(H.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Custom CNN classifier & {accuracy_score(y_test, predictions).round(4)} & {recall_score(y_test, predictions).round(4)} & {precision_score(y_test, predictions).round(4)} & {f1_score(y_test, predictions).round(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "tmp = 0\n",
    "idx = 0\n",
    "\n",
    "idx_m = 0\n",
    "idx_p = 0\n",
    "\n",
    "fig, axs = plt.subplots(4)\n",
    "for index,row in x_test.iterrows():\n",
    "    fig.set_size_inches(15, 20)\n",
    "    if(idx == 4):\n",
    "        break\n",
    "    if(predictions[item] == 0 and y_test[item] == 1 and idx_m < 2):\n",
    "        axs[idx].plot(row)\n",
    "        axs[idx].set_title(\"Predicted Noise | Actuals Spike\")\n",
    "        idx += 1\n",
    "        idx_m+=1\n",
    "    if(predictions[item] == 1 and y_test[item] == 0  and idx_p < 2):\n",
    "        axs[idx].plot(row)\n",
    "        axs[idx].set_title(\"Predicted Spike | Actuals Noise\")\n",
    "        idx += 1\n",
    "        idx_p+=1\n",
    "\n",
    "    item += 1\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_rnn(hp):\n",
    "    n_inputs = X_train_encode.shape[1]\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(Input(shape=(n_inputs,1)))\n",
    "    model.add(Bidirectional(LSTM(hp.Int(\"units1\", min_value=128, max_value=512, step=128), return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(hp.Int(\"units2\", min_value=64, max_value=256, step=64), return_sequences=True)))\n",
    "    model.add(Bidirectional(LSTM(hp.Int(\"units3\", min_value=32, max_value=128, step=32), return_sequences=True)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model with Adam optimizer\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                metrics=['accuracy'],\n",
    "                optimizer='Adam')\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = False\n",
    "if train:\n",
    "    tuner = keras_tuner.RandomSearch(\n",
    "        build_model_rnn,\n",
    "        objective='val_loss',\n",
    "        max_trials=5,\n",
    "        project_name=\"Classifier_RNN\")\n",
    "    tuner.search(X_train_encode, y_train, epochs=10, callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001),\n",
    "        ], validation_split=0.15)\n",
    "    bestHP = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "    # build the best model and train it\n",
    "    print(\"[INFO] training the best model...\")\n",
    "    model = tuner.hypermodel.build(bestHP)\n",
    "    H = model.fit(x=X_train_encode, y=y_train,\n",
    "        validation_split=0.15,\n",
    "        epochs=20, callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\"),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001),\n",
    "        ], verbose=1)\n",
    "else:\n",
    "    if os.path.exists('RNN_Classifier.h5'):\n",
    "        model = tf.keras.models.load_model('RNN_Classifier.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(x=X_test_encode, batch_size=32).round()\n",
    "print(classification_report(y_test,predictions))\n",
    "# generate the training loss/accuracy plot\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=y_test, y_pred=predictions)\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Oranges, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "   \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(H.history['loss'], label='train')\n",
    "plt.plot(H.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Custom RNN classifier & {accuracy_score(y_test, predictions).round(4)} & {recall_score(y_test, predictions).round(4)} & {precision_score(y_test, predictions).round(4)} & {f1_score(y_test, predictions).round(4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 0\n",
    "tmp = 0\n",
    "idx = 0\n",
    "\n",
    "idx_m = 0\n",
    "idx_p = 0\n",
    "\n",
    "fig, axs = plt.subplots(4)\n",
    "for index,row in x_test.iterrows():\n",
    "    fig.set_size_inches(15, 20)\n",
    "    if(idx == 4):\n",
    "        break\n",
    "    if(predictions[item] == 0 and y_test[item] == 1 and idx_m < 2):\n",
    "        axs[idx].plot(row)\n",
    "        axs[idx].set_title(\"Predicted Noise | Actuals Spike\")\n",
    "        idx += 1\n",
    "        idx_m+=1\n",
    "    if(predictions[item] == 1 and y_test[item] == 0  and idx_p < 2):\n",
    "        axs[idx].plot(row)\n",
    "        axs[idx].set_title(\"Predicted Spike | Actuals Noise\")\n",
    "        idx += 1\n",
    "        idx_p+=1\n",
    "\n",
    "    item += 1\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Aucun(e)",
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "937fe4b5d9b071df378a454ac50205e265fd9fba8afe26b93261d5b99b0f19da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
